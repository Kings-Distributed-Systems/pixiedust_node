{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Compute Protocol with Python and Node\n",
    "\n",
    "The idea behind this application is to demonstrate the unreasonable effectiveness of distributing tensorflow model inferencing and training jobs using a jupyter notebook with shared parameters with the python runtime. Allowing for data loading to be done in whatever language you're most familiar with and allowing you to parallelize whatever you may need to parallelize.\n",
    "\n",
    "![pixiedust_node](images/pixiedust_node_schematic.png)\n",
    "\n",
    "\n",
    "In the following cell, we initialize tensorflow, numpy and pixiedust_node. Pixiedust_node connects the ipython kernel to a node repl running in the background. \n",
    "\n",
    "We then initialize a few parameters for our identity keystore, our account keystore and the scheduler we'd like to target our jobs to go to. Addition, we clear the node instance `node.clear()`, we use `job-utility` to cancel all jobs running with our specific ID, Account and scheduler. `job-utility` must be installed globally using `npm install dcp-util -g`. Note that the exclamation marks are bash commands.\n",
    "\n",
    "Finally we install dcp-client in the specific npm location for the repl by using `npm.install`. \n",
    "\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "This is still an experimental tool and so sometimes pipes will die and consoles will go to the wrong place, please keep that in mind and just restart and clear all outputs if anything goes wrong.\n",
    "\n",
    "Pixiedust_node requires python 3.6 or below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pixiedust_node\n",
    "\n",
    "ID_KEY_LOC  = '/home/mgasmallah/DCP/keys/id.keystore'\n",
    "ACC_KEY_LOC = '/home/mgasmallah/DCP/keys/AISTEST.keystore'\n",
    "SCHEDULER    = 'https://demo-scheduler.distributed.computer'\n",
    "\n",
    "node.clear();\n",
    "!job-utility cancelAllJobs -I $ID_KEY_LOC --default-bank-account-file $ACC_KEY_LOC --scheduler $SCHEDULER\n",
    "npm.install(['dcp-client'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `%%node`\n",
    "\n",
    "Note that any cell with the magic `%%node` is now run on the node backend and any cell without this magic is in python and that any python variable of type `str`, `int`, `float`, `bool`, `dict`, or `list` will be moved to node when that cell is executed. Additionally, any variable declared with `var` in a `%%node` cell is automatically copied to the equivalent variable in python. The node and python variables are synced every second. \n",
    "\n",
    "Now that we have installed everything we need, we can initialize all the variables and values required for DCP to be used. In this case we add our ID key, account key and scheduler to the proccess argv in order to tell dcp where to find everything. Since there is no top level await, we simply use `initSync(process.argv)`. This allows us to now require `dcp/compute`, `dcp/wallet`, and `dcp/dcp-cli`. \n",
    "\n",
    "We also need an accountKeystore and an identityKeystore, however those are got asynchronously, so we'll initialize the variables here and fill them in later.\n",
    "\n",
    "This cell should only be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%node\n",
    "\n",
    "process.argv.push('-I', ID_KEY_LOC, '--default-bank-account-file',ACC_KEY_LOC, '--scheduler', SCHEDULER);\n",
    "require('dcp-client').initSync(process.argv)\n",
    "const compute = require('dcp/compute');\n",
    "const wallet = require('dcp/wallet');\n",
    "const dcpCli = require('dcp/dcp-cli');\n",
    "var identityKeystore;\n",
    "var accountKeystore;\n",
    "\n",
    "async function initKeystores(){\n",
    "    identityKeystore = await dcpCli.getIdentityKeystore();\n",
    "    accountKeystore  = await dcpCli.getAccountKeystore();\n",
    "    wallet.addId(identityKeystore);\n",
    "}\n",
    "\n",
    "await initKeystores();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "This is the first concrete simple example of distributing a job using DCP in a node cell. Since this process is asynchronous and doesn't quite sync up with ipython kernel, when this is run, you should wait until you have fully completed execution of the function. \n",
    "\n",
    "We begin by geting our identity keystore and our account keystore. Once these are loaded in we initialize our job using `compute.for`. `compute.for` takes an array of which each slice will be sent to a worker as an argument to the worker function. The worker function is the second argument and is what will be executed on each worker.\n",
    "\n",
    "We throw in some `job.on` overrides so we can log our progress through distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%node\n",
    "\n",
    "async function main(){\n",
    "    let job = compute.for([...Array(1).keys()], async function(sim_id){\n",
    "        progress(0.01);\n",
    "        console.log(\"Beginning require with index: \", sim_id);\n",
    "        const tf = require('aistensorflow/tfjs');\n",
    "        console.log(tf.version);\n",
    "        progress(1.0);\n",
    "        return \"done\";\n",
    "    });\n",
    "\n",
    "    job.on('accepted', ()=>{\n",
    "        console.log('Job accepted...');\n",
    "    });\n",
    "    job.on('status', (status)=>{\n",
    "        console.log('Got a status update: ', status);\n",
    "    });\n",
    "    job.on('console', (output)=>{\n",
    "        console.log(output.message);\n",
    "    });\n",
    "    job.on('error', (err)=>{\n",
    "        console.log(err);\n",
    "    });\n",
    "    job.requires('aistensorflow/tfjs');\n",
    "    job.public.name = 'dcp-testing';\n",
    "    console.log(\"Launching job\");\n",
    "    await job.exec(compute.marketValue, accountKeystore);\n",
    "    console.log(\"Done executing job\");\n",
    "};\n",
    "\n",
    "await main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python preprocessing\n",
    "\n",
    "Now let's demonstrate the neat things we can do with python-js bridging. Here we take advantage of the python tensorflow implementation which comes with mnist. We load this data in and do our data preprocessing in python.\n",
    "\n",
    "Once we have finished our data preprocessing, we convert the data to a list and initialize our results array in python as well. This allows us to sync to the js instance in the back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#Change datatype\n",
    "x_train = x_train.astype(np.uint8)\n",
    "x_test  = x_test.astype(np.uint8)\n",
    "\n",
    "plt.imshow(x_train[0,...], cmap='gray')\n",
    "plt.title(\"Sample digit\")\n",
    "plt.show()\n",
    "\n",
    "#change [60000] to [60000,10] (one_hot)\n",
    "y_train = tf.one_hot(y_train, 10).numpy()\n",
    "y_test  = tf.one_hot(y_test, 10).numpy()\n",
    "\n",
    "#reshape from a [-1,28,28] to a [-1,783] \n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test  = x_test.reshape(-1, 784)\n",
    "\n",
    "#We choose the first 1000 samples for training and reshape to [-1] and convert to a list\n",
    "x_train = x_train\n",
    "y_train = y_train\n",
    "#initialize a list to use in js and python\n",
    "mnistResults = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we confirm that the arrays have been send to the js instance by logging their type and their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%node\n",
    "console.log(x_train.shape, x_train.dtype, y_train.shape, y_train.dtype);\n",
    "console.log(mnistResults);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split up our data into batches to send off to the protocol for training. This is (for the majority) the same as previous main function, but does a few things more in splitting up the data and in adding to the work function. In this example we use the `job.requires` syntax to require tensorflowjs in each of our workers. Although you could `npm.install( '@tensorflow/tfjs')` and `job.requires('@tensorflow/tfjs')`, we provide a module on DCP called `aistensorflow/tfjs` which has all of tfjs and is confirmed working for DCP.\n",
    "\n",
    "Finally, note that we have added on the `job.on(results,()....` a push to the `mnistResults` variable declared in python above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%node\n",
    "async function main(){\n",
    "    let xtrain = x_train.typedArray;\n",
    "    let ytrain = y_train.typedArray;\n",
    "    let batch = 32;\n",
    "    \n",
    "    \n",
    "    let trainingArray = [];\n",
    "    \n",
    "    for (let i=0;i< batch*5;i+=batch){\n",
    "        let xs = Array.from(xtrain.slice(i*784, Math.min(xtrain.length, (i+batch)*784)));\n",
    "        let ys = Array.from(ytrain.slice(i*10, Math.min(ytrain.length, (i+batch)*10)));\n",
    "        trainingArray.push({xs, ys, batch});\n",
    "    }\n",
    "    \n",
    "    let job = compute.for(trainingArray, async function(data){\n",
    "        progress();\n",
    "        const tf = require('aistensorflow/tfjs');\n",
    "        tf.setBackend('cpu');\n",
    "        await tf.ready();\n",
    "        \n",
    "        const model = tf.sequential({\n",
    "            layers: [\n",
    "                tf.layers.dense({inputShape: [784], units:32, activation: 'relu'}),\n",
    "                tf.layers.dense({units: 64, activation: 'relu'}),\n",
    "                tf.layers.dense({units: 128, activation: 'relu'}),\n",
    "                tf.layers.dense({units: 10, activation: 'softmax'})\n",
    "            ]\n",
    "        });\n",
    "        progress();\n",
    "        model.compile({\n",
    "            optimizer: 'sgd',\n",
    "            loss: 'categoricalCrossentropy',\n",
    "            metrics: ['accuracy']\n",
    "        })\n",
    "    \n",
    "        let xTrain = tf.tensor(data.xs, [data.xs.length/784, 784], dtype='float32');\n",
    "        let yTrain = tf.tensor(data.ys, [data.ys.length/10, 10], dtype='int32');\n",
    "        \n",
    "        const history = await model.fit(xTrain, yTrain, {\n",
    "            epochs: 5,\n",
    "            callbacks: {\n",
    "                onEpochEnd: async (epochs, logs)=>{\n",
    "                    progress();\n",
    "                }\n",
    "            }\n",
    "        });\n",
    "        \n",
    "        xTrain.dispose();\n",
    "        yTrain.dispose();\n",
    "        progress(1.0);\n",
    "        return history.history.acc;\n",
    "    });\n",
    "\n",
    "    job.on('accepted', ()=>{\n",
    "        console.log('Job accepted...');\n",
    "    });\n",
    "    job.on('status', (status)=>{\n",
    "        console.log('Got a status update: ', status);\n",
    "    });\n",
    "    job.on('result', (value)=>{\n",
    "        let result = value.result;\n",
    "        mnistResults.push(result[result.length-1]);\n",
    "        \n",
    "    });\n",
    "    job.on('console', (output)=>{\n",
    "        console.log(output.message);\n",
    "    });\n",
    "    job.on('error', (err)=>{\n",
    "        console.log(err);\n",
    "    });\n",
    "    job.requires('aistensorflow/tfjs');\n",
    "    job.public.name = 'dcp-vae-testing';\n",
    "    console.log(\"Launching job\");\n",
    "    await job.exec(compute.marketValue, accountKeystore);\n",
    "    console.log(\"Done executing job\");\n",
    "};\n",
    "\n",
    "await main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Visualization \n",
    "Now that we have all these results and they are available in python, we can do some very quick data visualization in python! Here we use matplotlibs pyplot to plot the accuracies of each worker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mnistResults)\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow to DCP\n",
    "\n",
    "In this section we will train a model in python, convert it to js and ship it off to DCP as a module that we can require. First we begin by requiring the python tensorflowjs package which allows for python to js conversion of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflowjs as tfjs\n",
    "except:\n",
    "    !pip install tensorflowjs tf-estimator-nightly\n",
    "    import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply use the mnist dataset to train a simple network on mnist in python and evaluate it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n",
    "del x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using tensorflowjs in python and a tensorflowjs utility we have built, we can save the model to a directory and publish this model to DCP. Note that the `-p` package version number must be incremented everytime if the package name `dcp_mnist_ex/mnist.js` is not changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir './tfjs_model'\n",
    "tfjs.converters.save_keras_model(model, './tfjs_model')\n",
    "!ls './tfjs_model'\n",
    "!node ~/DCP/dcp-utils/tfjs_utils/bin/serializeModel.js -m ./tfjs_model/model.json -o dcp_mnist_ex/mnist.js -p 1.0.10 -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to use this model is pretty trivial. We use the same `x_train`and `y_train` variables in node that we had from before and batch them for inference using our model. We require `aistensorflow/tfjs` and `dcp_mnist_ex/mnist.js` as well as `dcp-polyfills/polyfills.js`. This allows us to `await require('mnist').getModel()` to get the mnist model we built. In order to evaluate we have to compile the model with a loss and an accuracy. Finally we evaluate the model and return our accuracy. We have reset mnistResults and so we can use it to fill it in with our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%node\n",
    "mnistResults = [];\n",
    "async function main(){\n",
    "    let xtrain = Uint8Array.from(x_train);\n",
    "    let ytrain = Uint8Array.from(y_train);\n",
    "    let batch = 32;\n",
    "    \n",
    "    \n",
    "    let trainingArray = [];\n",
    "    \n",
    "    for (let i=0;i< 10;i+=batch){\n",
    "        let xs = Array.from(xtrain.slice(i*784, Math.min(xtrain.length, (i+batch)*784)));\n",
    "        let ys = Array.from(ytrain.slice(i*10, Math.min(ytrain.length, (i+batch)*10)));\n",
    "        trainingArray.push({xs, ys, batch});\n",
    "    }\n",
    "    \n",
    "    let job = compute.for(trainingArray, async function(data){\n",
    "        progress();\n",
    "        require('polyfills');\n",
    "        const tf = require('aistensorflow/tfjs');\n",
    "        tf.setBackend('cpu');\n",
    "        await tf.ready();\n",
    "        \n",
    "        const model = await require('mnist').getModel();\n",
    "        \n",
    "        let xTrain = tf.tensor(data.xs, [data.xs.length/784, 28, 28], dtype='float32');\n",
    "        let yTrain = tf.tensor(data.ys, [data.ys.length/10, 10], dtype='int32');\n",
    "        \n",
    "        \n",
    "        model.compile({\n",
    "            optimizer: 'sgd',\n",
    "            loss: 'categoricalCrossentropy',\n",
    "            metrics: ['accuracy']\n",
    "        })\n",
    "        \n",
    "        const result = model.evaluate(xTrain, yTrain, {\n",
    "            batchSize: 32,\n",
    "        })[1];\n",
    "        \n",
    "        progress(1.0);\n",
    "        return result.dataSync()[0];\n",
    "    });\n",
    "\n",
    "    job.on('accepted', ()=>{\n",
    "        console.log('Job accepted...');\n",
    "    });\n",
    "    job.on('status', (status)=>{\n",
    "        console.log('Got a status update: ', status);\n",
    "    });\n",
    "    job.on('result', (value)=>{\n",
    "        mnistResults.push(value.result);\n",
    "        \n",
    "    });\n",
    "    job.on('console', (output)=>{\n",
    "        console.log(output.message);\n",
    "    });\n",
    "    job.on('error', (err)=>{\n",
    "        console.log(err);\n",
    "    });\n",
    "    \n",
    "    job.requires('dcp-polyfills/polyfills');\n",
    "    job.requires('aistensorflow/tfjs');\n",
    "    job.requires('dcp_mnist_ex/mnist');\n",
    "    \n",
    "    \n",
    "    job.public.name = 'dcp-vae-testing';\n",
    "    console.log(\"Launching job\");\n",
    "    await job.exec(compute.marketValue, accountKeystore);\n",
    "    console.log(\"Done executing job\");\n",
    "};\n",
    "\n",
    "await main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for entire set: \", sum(mnistResults)/len(mnistResults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import pixiedust_node\n",
    "except:\n",
    "    !pip install pixiedust pixiedust_node\n",
    "    import pixiedust_node\n",
    "\n",
    "ID_KEY_LOC  = '/home/mgasmallah/DCP/keys/office/id.keystore'#'/home/mgasmallah/DCP/keys/id.keystore'\n",
    "ACC_KEY_LOC = '/home/mgasmallah/DCP/keys/office/default.keystore'#'/home/mgasmallah/DCP/keys/AISTEST.keystore'\n",
    "SCHEDULER    = 'http://scheduler.hamada.office.kingsds.network/'#'https://demo-scheduler.distributed.computer'\n",
    "\n",
    "node.clear();\n",
    "!job-utility cancelAllJobs -I $ID_KEY_LOC --default-bank-account-file $ACC_KEY_LOC --scheduler $SCHEDULER\n",
    "npm.install( 'dcp-client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%node\n",
    "\n",
    "process.argv.push('-I', ID_KEY_LOC, '--default-bank-account-file',ACC_KEY_LOC, '--scheduler', SCHEDULER);\n",
    "require('dcp-client').initSync(process.argv)\n",
    "const tf = require('@tensorflow/tfjs');\n",
    "const compute = require('dcp/compute');\n",
    "const wallet = require('dcp/wallet');\n",
    "const dcpCli = require('dcp/dcp-cli');\n",
    "var accountKeystore;\n",
    "var identityKeystore;\n",
    "\n",
    "(async function main(){\n",
    "    identityKeystore = await dcpCli.getIdentityKeystore();\n",
    "    wallet.addId(identityKeystore);\n",
    "    accountKeystore = await dcpCli.getAccountKeystore();\n",
    "})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = np.expand_dims(train_images,axis=-1) / 255.\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1).tolist()\n",
    "latentDims = 10\n",
    "numIterations = 2\n",
    "numEpochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%node\n",
    "\n",
    "mnistResults = [];\n",
    "\n",
    "function getWeights(model){\n",
    "    return tf.tidy(()=>{\n",
    "        let w = model.getWeights();\n",
    "        let retWeights = [];\n",
    "        for (let i = 0; i < w.length; i++){\n",
    "            retWeights.push({data: w[i].arraySync(), shape: w[i].shape });\n",
    "        }\n",
    "        return retWeights;\n",
    "    });\n",
    "}\n",
    "\n",
    "function setWeights(model, weightsObj){\n",
    "    return tf.tidy(()=>{\n",
    "        let w = [];\n",
    "        for (let i = 0; i < weightsObj.length; i++){\n",
    "            w.push(tf.tensor(weightsObj[i].data, weightsObj[i].shape));\n",
    "        }\n",
    "        model.setWeights(w);\n",
    "        return model;\n",
    "    });\n",
    "}\n",
    "\n",
    "function averageWeights(ws){\n",
    "    return tf.tidy(()=>{\n",
    "        let outWeights = [];\n",
    "        for (let i = 0; i < ws[0].length; i++){\n",
    "            let w = [];\n",
    "            for (let j = 0; j < ws.length; j++){\n",
    "                w.push(ws[j][i].data);\n",
    "            }\n",
    "            let shape = ws[0][i].shape;\n",
    "            shape = [w.length, ...shape];\n",
    "            w = tf.tensor(w, shape).mean(axis=0);\n",
    "            outWeights.push({data: w.arraySync(), shape: w.shape });\n",
    "        }\n",
    "        return outWeights;\n",
    "    });\n",
    "}\n",
    "\n",
    "\n",
    "async function main(){\n",
    "    \n",
    "    let xtrain = Float32Array.from(train_images);\n",
    "    let batch = 32;\n",
    "\n",
    "    let trainingArray = [];\n",
    "    let allEncoderWeights = [];\n",
    "    let allDecoderWeights = [];\n",
    "    \n",
    "    for (let i=0;i< Math.floor(xtrain.length/784);i+=batch){\n",
    "        let xs = Array.from(xtrain.slice(i*784, Math.min(xtrain.length, (i+batch)*784)));\n",
    "        trainingArray.push({xs, latentDims, numIterations, batch});\n",
    "    }\n",
    "    \n",
    "    \n",
    "    let encoder = tf.sequential({\n",
    "        layers: [\n",
    "            tf.layers.conv2d({filters: 32, kernelSize: 3, strides: 2, activation: 'relu', inputShape: [28, 28, 1]}),\n",
    "            tf.layers.conv2d({filters: 64, kernelSize: 3, strides: 2, activation: 'relu'}),\n",
    "            tf.layers.flatten(),\n",
    "            tf.layers.dense({units: latentDims + latentDims })\n",
    "        ]\n",
    "    });\n",
    "    \n",
    "    let decoder = tf.sequential({\n",
    "        layers: [\n",
    "            tf.layers.dense({ units: 7*7*32, activation: 'relu', inputShape: [latentDims]}),\n",
    "            tf.layers.reshape({ targetShape: [7,7,32]}),\n",
    "            tf.layers.conv2dTranspose({filters: 64, kernelSize: 3, strides: 2, padding: 'same',\n",
    "                                      activation: 'relu'}),\n",
    "            tf.layers.conv2dTranspose({filters: 32, kernelSize: 3, strides: 2, padding: 'same',\n",
    "                                      activation: 'relu'}),\n",
    "            tf.layers.conv2dTranspose({filters: 1, kernelSize: 3, strides: 1, padding: 'same'}),\n",
    "        ]\n",
    "    });\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for (let e = 0; e < numEpochs; e++){\n",
    "    \n",
    "        let encoderWeights = getWeights(encoder);\n",
    "        let decoderWeights = getWeights(decoder);\n",
    "        \n",
    "        let job = compute.for(trainingArray, async function(data, eWeights, dWeights){\n",
    "            progress();\n",
    "            require('polyfills');\n",
    "            const tf = require('aistensorflow/tfjs');\n",
    "            tf.setBackend('cpu');\n",
    "            await tf.ready();\n",
    "            \n",
    "            function getWeights(model){\n",
    "                return tf.tidy(()=>{\n",
    "                    let w = model.getWeights();\n",
    "                    let retWeights = [];\n",
    "                    for (let i = 0; i < w.length; i++){\n",
    "                        retWeights.push({data: w[i].arraySync(), shape: w[i].shape });\n",
    "                    }\n",
    "                    return retWeights;\n",
    "                });\n",
    "            };\n",
    "            \n",
    "            function setWeights(model, weightsObj){\n",
    "                return tf.tidy(()=>{\n",
    "                    let w = [];\n",
    "                    for (let i = 0; i < weightsObj.length; i++){\n",
    "                        w.push(tf.tensor(weightsObj[i].data, weightsObj[i].shape));\n",
    "                    }\n",
    "                    model.setWeights(w);\n",
    "                    return model;\n",
    "                });\n",
    "            }\n",
    "            \n",
    "            progress();\n",
    "            \n",
    "            let encoder = tf.sequential({\n",
    "                layers: [\n",
    "                    tf.layers.conv2d({filters: 32, kernelSize: 3, strides: 2, activation: 'relu', inputShape: [28, 28, 1]}),\n",
    "                    tf.layers.conv2d({filters: 64, kernelSize: 3, strides: 2, activation: 'relu'}),\n",
    "                    tf.layers.flatten(),\n",
    "                    tf.layers.dense({units: data.latentDims + data.latentDims })\n",
    "                ]\n",
    "            });\n",
    "            progress();\n",
    "            let decoder = tf.sequential({\n",
    "                layers: [\n",
    "                    tf.layers.dense({ units: 7*7*32, activation: 'relu', inputShape: [data.latentDims]}),\n",
    "                    tf.layers.reshape({ targetShape: [7,7,32]}),\n",
    "                    tf.layers.conv2dTranspose({filters: 64, kernelSize: 3, strides: 2, padding: 'same',\n",
    "                                              activation: 'relu'}),\n",
    "                    tf.layers.conv2dTranspose({filters: 32, kernelSize: 3, strides: 2, padding: 'same',\n",
    "                                              activation: 'relu'}),\n",
    "                    tf.layers.conv2dTranspose({filters: 1, kernelSize: 3, strides: 1, padding: 'same'}),\n",
    "                ]\n",
    "            });\n",
    "            progress();\n",
    "            \n",
    "            \n",
    "            encoder = setWeights(encoder, eWeights);\n",
    "            decoder = setWeights(decoder, dWeights);\n",
    "            \n",
    "            \n",
    "            const encode = (x)=>{\n",
    "                progress();\n",
    "                const [ mean, logvar ] = tf.split(encoder.predict(x), 2, 1);\n",
    "                progress();\n",
    "                return  [ mean, logvar ];            \n",
    "            };\n",
    "            const decode = (z, applySigmoid)=>{\n",
    "                progress();\n",
    "                let logits = decoder.predict(z);\n",
    "                progress();\n",
    "                if (applySigmoid){\n",
    "                    progress();\n",
    "                    let probs = tf.sigmoid(logits);\n",
    "                    return probs;\n",
    "                }\n",
    "                return logits;\n",
    "            }\n",
    "\n",
    "\n",
    "            progress();\n",
    "            const sample = (eps)=>{\n",
    "                progress();\n",
    "                if (typeof eps === 'undefined'){\n",
    "                    eps = tf.randomNormal([32, data.latentDims]);\n",
    "                }\n",
    "                return decode(eps, applySigmoid=true);\n",
    "            };\n",
    "\n",
    "            const reparameterize = (mean, logvar)=>{\n",
    "                progress();\n",
    "                let eps = tf.randomNormal([mean.shape[0], data.latentDims]);\n",
    "                progress();\n",
    "                return eps.mul(tf.exp(logvar.mul(.5))).add(mean);\n",
    "            };\n",
    "\n",
    "            const logNormalPdf = (sample, mean, logvar, raxis=1)=>{\n",
    "                progress();\n",
    "                let log2pi = tf.log(Math.PI * 2.)\n",
    "                let diff = (sample.sub(mean)).pow(2);\n",
    "                progress();\n",
    "                diff = diff.mul(-.5);\n",
    "                diff = diff.mul(tf.exp(logvar.mul(-1)));\n",
    "                progress();\n",
    "                diff = diff.add(logvar);\n",
    "                diff = diff.add(log2pi);\n",
    "                progress();\n",
    "                return tf.sum(diff, raxis);\n",
    "            }; \n",
    "\n",
    "            const computeLoss = (x)=>{\n",
    "                return tf.tidy(()=>{\n",
    "                    progress();\n",
    "                    const [mean, logvar] = encode(x);\n",
    "                    progress();\n",
    "                    const z = reparameterize( mean, logvar);\n",
    "                    progress();\n",
    "                    const xLogit = decode(z);\n",
    "                    progress();\n",
    "                    const crossEntropy = tf.losses.sigmoidCrossEntropy(x, xLogit, undefined, undefined, tf.Reduction.NONE);\n",
    "                    progress();\n",
    "                    const logpxZ = tf.sum(crossEntropy, axis=[1,2,3]).mul(-1);\n",
    "                    progress();\n",
    "                    const logpz = logNormalPdf(z, tf.tensor(0.),tf.tensor(0.));\n",
    "                    progress();\n",
    "                    const logqzX = logNormalPdf(z, mean, logvar);\n",
    "                    progress();\n",
    "                    let loss = logpxZ.add(logpz);\n",
    "                    progress();\n",
    "                    loss = loss.sub(logqzX);\n",
    "                    progress();\n",
    "                    loss = tf.mean(loss).mul(-1);\n",
    "                    progress();\n",
    "                    return loss;\n",
    "                });\n",
    "            };\n",
    "\n",
    "            const optimizer = tf.train.adam(0.0001)\n",
    "\n",
    "            let l = 0;\n",
    "\n",
    "            let xs = tf.tensor(data.xs, [data.xs.length/784, 28,28,1], dtype='float32');\n",
    "            for (let i=0; i< data.numIterations; i++){\n",
    "                l = 0;\n",
    "                l += optimizer.minimize(()=>computeLoss(xs), true);\n",
    "                progress();\n",
    "            }\n",
    "            progress();\n",
    "            eWeights = getWeights(encoder);\n",
    "            progress();\n",
    "            dWeights = getWeights(decoder);\n",
    "            console.log(\"Done slice\");\n",
    "            progress(1.0);\n",
    "            \n",
    "            return { loss: l.arraySync(), eWeights, dWeights };\n",
    "        }, [encoderWeights, decoderWeights]);\n",
    "\n",
    "        job.on('accepted', ()=>{\n",
    "            console.log('Job accepted...');\n",
    "        });\n",
    "        job.on('status', (status)=>{\n",
    "            console.log('Got a status update: ', status);\n",
    "        });\n",
    "        job.on('result', (value)=>{\n",
    "            allEncoderWeights.push(value.result.eWeights);\n",
    "            allDecoderWeights.push(value.result.dWeights);\n",
    "        });\n",
    "        job.on('console', (output)=>{\n",
    "            console.log(output.message);\n",
    "        });\n",
    "        job.on('error', (err)=>{\n",
    "            console.log(err);\n",
    "        });\n",
    "\n",
    "        //job.requirements.environment.offscreenCanvas = true;\n",
    "\n",
    "        job.requires('dcp-polyfills/polyfills');\n",
    "        job.requires('aistensorflow/tfjs');\n",
    "\n",
    "\n",
    "        job.public.name = 'dcp-vae-testing';\n",
    "        console.log(\"Launching job\");\n",
    "        let results = await job.exec(compute.marketValue, accountKeystore);\n",
    "        \n",
    "        let avgEncWeights = averageWeights(allEncoderWeights);\n",
    "        let avgDecWeights = averageWeights(allDecoderWeights);\n",
    "        \n",
    "        encoder = setWeights(encoder, avgEncWeights);\n",
    "        decoder = setWeights(decoder, avgDecWeights);\n",
    "    }\n",
    "    console.log(\"Done executing job\");\n",
    "};\n",
    "\n",
    "main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(mnistResults[3]['img'])[0,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
